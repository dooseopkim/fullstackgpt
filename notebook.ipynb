{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xdrlib import ConversionError\n",
    "\n",
    "\"\"\"\n",
    "#6.7 Recap (05:17)\n",
    "#6.8 Stuff LCEL Chain (10:13)\n",
    "\n",
    "1. File들을 Load 하기.\n",
    " - TextLoader, UnstructuredFileLoader\n",
    "2. Split (거대한 단일 document 보다 작은 여러개를 LLM 전달할 때 검색 성능이 좋다\n",
    "3. Ebmedding : text에 의미별로 적절한 점수를 부여해서 vector 형식으로 표현한거\n",
    " - O\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일을 환경 변수로 로드"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T16:21:41.013422400Z",
     "start_time": "2025-06-02T16:21:40.994758100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# chunk_size : 얼마나 큰 덩어리로 나눌지 결정할 수 있음\n",
    "# chunk_overlap : 문장이나 문단을 분할할 때 앞 조각 일부분을 가져옴\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/현진건-운수_좋은_날.txt\",encoding=\"utf-8\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T16:21:42.093596500Z",
     "start_time": "2025-06-02T16:21:41.515427300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=20,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T16:21:43.715460100Z",
     "start_time": "2025-06-02T16:21:43.707445500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    print(result)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T16:21:47.975513100Z",
     "start_time": "2025-06-02T16:21:47.964512400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='김첨지는 구걸하는 거지나 무엇같이 연해연방인 것 같은 기생 퇴물로 묘사되어 있습니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"김첨지의 직업은 무엇?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T16:22:14.662765200Z",
     "start_time": "2025-06-02T16:22:11.268509Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_memory({})"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
